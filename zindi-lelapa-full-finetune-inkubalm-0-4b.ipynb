{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11022448,"sourceType":"datasetVersion","datasetId":6863832}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U trl bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:06.828062Z","iopub.execute_input":"2025-05-06T16:18:06.828341Z","iopub.status.idle":"2025-05-06T16:18:14.673599Z","shell.execute_reply.started":"2025-05-06T16:18:06.828318Z","shell.execute_reply":"2025-05-06T16:18:14.672511Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.3.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.47.0)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.11.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl, bitsandbytes\nSuccessfully installed bitsandbytes-0.45.5 trl-0.17.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom datasets import load_dataset, concatenate_datasets, Dataset, Value\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:14.674970Z","iopub.execute_input":"2025-05-06T16:18:14.675309Z","iopub.status.idle":"2025-05-06T16:18:41.634447Z","shell.execute_reply.started":"2025-05-06T16:18:14.675267Z","shell.execute_reply":"2025-05-06T16:18:41.633550Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"HFtoken\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:41.635903Z","iopub.execute_input":"2025-05-06T16:18:41.636151Z","iopub.status.idle":"2025-05-06T16:18:41.825821Z","shell.execute_reply.started":"2025-05-06T16:18:41.636132Z","shell.execute_reply":"2025-05-06T16:18:41.825160Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Define reusable functions for dataset loading and processing","metadata":{}},{"cell_type":"code","source":"def load_dataset_by_tag(dataset_type, tag, split='train'):\n    return load_dataset(f\"{dataset_type}{tag}\", split=split)\n\ndef load_and_combine_datasets(tag, split='train'):\n    \"\"\"\n    Load and combine multiple datasets with all unique columns (union).\n    Empty strings are used for missing values.\n    \n    Args:\n        tag (str): Tag for the datasets (Train, Test)\n        split (str): Split to load (train, test)\n        \n    Returns:\n        Dataset: Combined dataset with all unique columns\n    \"\"\"\n    se_dataset = load_dataset_by_tag(\"lelapa/Sentiment\", tag, split)\n    mt_dataset = load_dataset_by_tag(\"lelapa/MT\", tag, split)\n    xn_dataset = load_dataset_by_tag(\"lelapa/XNLI\", tag, split)\n\n    # Identify all unique columns (union)\n    all_columns = list(set(se_dataset.column_names) | \n                      set(mt_dataset.column_names) | \n                      set(xn_dataset.column_names))\n    print(f\"All Columns: {all_columns}\")\n\n    # Function to ensure dataset has all columns, filling missing ones with empty strings\n    def ensure_all_columns(dataset, all_cols):\n        # Add each missing column one by one\n        for col in all_cols:\n            if col not in dataset.column_names:\n                # Create array of empty strings with the same length as the dataset\n                empty_column = [\"\"] * len(dataset)\n                dataset = dataset.add_column(col, empty_column)\n        \n        return dataset\n\n    # Ensure all datasets have all columns\n    se_dataset = ensure_all_columns(se_dataset, all_columns)\n    mt_dataset = ensure_all_columns(mt_dataset, all_columns)\n    xn_dataset = ensure_all_columns(xn_dataset, all_columns)\n\n    # Make sure 'targets' column is string type if it exists in all datasets\n    if \"targets\" in all_columns:\n        se_dataset = se_dataset.cast_column(\"targets\", Value(\"string\"))\n        mt_dataset = mt_dataset.cast_column(\"targets\", Value(\"string\"))\n        xn_dataset = xn_dataset.cast_column(\"targets\", Value(\"string\"))\n\n    # Concatenate datasets\n    combined_dataset = concatenate_datasets([se_dataset, mt_dataset, xn_dataset])\n\n    return combined_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:41.827106Z","iopub.execute_input":"2025-05-06T16:18:41.827401Z","iopub.status.idle":"2025-05-06T16:18:42.448654Z","shell.execute_reply.started":"2025-05-06T16:18:41.827370Z","shell.execute_reply":"2025-05-06T16:18:42.447764Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train = load_and_combine_datasets('train')\npd.DataFrame(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:42.449508Z","iopub.execute_input":"2025-05-06T16:18:42.449777Z","iopub.status.idle":"2025-05-06T16:18:48.353620Z","shell.execute_reply.started":"2025-05-06T16:18:42.449748Z","shell.execute_reply":"2025-05-06T16:18:48.352908Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/485 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f71fe342aa4826aacf5deda34f1c54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/39.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1972d99f1eed4671b6c2e4ae64d38f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee40abbf7454c909157985cf7504030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/485 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d31594a72a044dde85247dd701c07ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/72.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e4e23425484a8392cec5309719fa32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6ecc9847a74994baa5abb5a81e7d75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/447 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015b9673ea584893a5b17b1c44e471ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/35.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a362ca6947d14fbc895d4a3f372b3d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75aca82b522b46f09cc4f085e07715b9"}},"metadata":{}},{"name":"stdout","text":"All Columns: ['task', 'data_source', 'instruction', 'targets', 'premise', 'inputs', 'langs', 'ID']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82095cfac2924aa2963a69a477cd07b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf5e7bdc8294d66ba239e3d904e5690"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf53bf048124c4099df4b4201752d4f"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                      ID       task    langs    data_source  \\\n0       ID_6aba33a1_sentiment_ dev_hausa  sentiment    hausa      afrisenti   \n1       ID_ce64d307_sentiment_ dev_hausa  sentiment    hausa     naijasenti   \n2     ID_dfb02831_sentiment_ dev_swahili  sentiment  swahili  swahili_tweet   \n3       ID_2efc9515_sentiment_ dev_hausa  sentiment    hausa      afrisenti   \n4     ID_ad1d9888_sentiment_ dev_swahili  sentiment  swahili      afrisenti   \n...                                  ...        ...      ...            ...   \n1395        ID_085354e1_dev_afrixnli_swa                 swa                  \n1396        ID_586e104a_dev_afrixnli_swa                 swa                  \n1397        ID_b871ea53_dev_afrixnli_hau                 hau                  \n1398        ID_70aae970_dev_afrixnli_hau                 hau                  \n1399        ID_9d731aca_dev_afrixnli_swa                 swa                  \n\n                                            instruction  \\\n0     Za ka iya tantance yanayin wannan rubutu? Bi w...   \n1     Da fatan za a gano ra'ayin da ke cikin wannan ...   \n2     Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n3     Gano ra'ayin da aka bayyana a cikin wannan rub...   \n4     Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...   \n...                                                 ...   \n1395  Is the following question True, False or Neither?   \n1396  Is the following question True, False or Neither?   \n1397  Is the following question True, False or Neither?   \n1398  Is the following question True, False or Neither?   \n1399  Is the following question True, False or Neither?   \n\n                                                 inputs      targets  \\\n0             @user @user allah ya tsayyaba yar uwa ğŸ‚ ğŸ˜    Kyakkyawa   \n1     @user intenet a masallachiğŸ˜­ğŸ˜­ğŸ˜­ wani salo ne na ...  Tsaka-tsaki   \n2     picha mbunge wa kilombero peter lijualikali ak...      Wastani   \n3     @user @user @user @user @user hhh amma rahama ...  Tsaka-tsaki   \n4     swali zuri sana nawatafuta wajuzi wa mambo wat...      Wastani   \n...                                                 ...          ...   \n1395  Maduka ya habari ya kitaifa hufanya maeneo yet...            1   \n1396  Kurasa zilihusisha wanachama na maafisa wa kaw...            0   \n1397  Ban damu ba da abinda labarun Æ™asa ke nuni cik...            2   \n1398  Ya zabi Æ™in kama hannayen sa saboda anyi musu ...            0   \n1399  Steve Harris alikataa kutoka kwa nyumba yake k...            2   \n\n                                                premise  \n0                                                        \n1                                                        \n2                                                        \n3                                                        \n4                                                        \n...                                                 ...  \n1395  tahadhari kuhusu jinsi habari za kitaifa zinav...  \n1396  Uanachama ulijumuisha  kati ya wanaume wazima ...  \n1397  Ka damu da yadda labarun Æ™asa ke shafar unguwa...  \n1398  Kuma mani rashin mutunci dan bazan goyi banyan...  \n1399  Steve Harris, mwana biolojia wa molekuli kutok...  \n\n[1400 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>task</th>\n      <th>langs</th>\n      <th>data_source</th>\n      <th>instruction</th>\n      <th>inputs</th>\n      <th>targets</th>\n      <th>premise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_6aba33a1_sentiment_ dev_hausa</td>\n      <td>sentiment</td>\n      <td>hausa</td>\n      <td>afrisenti</td>\n      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n      <td>@user @user allah ya tsayyaba yar uwa ğŸ‚ ğŸ˜</td>\n      <td>Kyakkyawa</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_ce64d307_sentiment_ dev_hausa</td>\n      <td>sentiment</td>\n      <td>hausa</td>\n      <td>naijasenti</td>\n      <td>Da fatan za a gano ra'ayin da ke cikin wannan ...</td>\n      <td>@user intenet a masallachiğŸ˜­ğŸ˜­ğŸ˜­ wani salo ne na ...</td>\n      <td>Tsaka-tsaki</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_dfb02831_sentiment_ dev_swahili</td>\n      <td>sentiment</td>\n      <td>swahili</td>\n      <td>swahili_tweet</td>\n      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n      <td>picha mbunge wa kilombero peter lijualikali ak...</td>\n      <td>Wastani</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_2efc9515_sentiment_ dev_hausa</td>\n      <td>sentiment</td>\n      <td>hausa</td>\n      <td>afrisenti</td>\n      <td>Gano ra'ayin da aka bayyana a cikin wannan rub...</td>\n      <td>@user @user @user @user @user hhh amma rahama ...</td>\n      <td>Tsaka-tsaki</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_ad1d9888_sentiment_ dev_swahili</td>\n      <td>sentiment</td>\n      <td>swahili</td>\n      <td>afrisenti</td>\n      <td>Tafadhali tambua mawazo yaliyoonyeshwa kwenye ...</td>\n      <td>swali zuri sana nawatafuta wajuzi wa mambo wat...</td>\n      <td>Wastani</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1395</th>\n      <td>ID_085354e1_dev_afrixnli_swa</td>\n      <td></td>\n      <td>swa</td>\n      <td></td>\n      <td>Is the following question True, False or Neither?</td>\n      <td>Maduka ya habari ya kitaifa hufanya maeneo yet...</td>\n      <td>1</td>\n      <td>tahadhari kuhusu jinsi habari za kitaifa zinav...</td>\n    </tr>\n    <tr>\n      <th>1396</th>\n      <td>ID_586e104a_dev_afrixnli_swa</td>\n      <td></td>\n      <td>swa</td>\n      <td></td>\n      <td>Is the following question True, False or Neither?</td>\n      <td>Kurasa zilihusisha wanachama na maafisa wa kaw...</td>\n      <td>0</td>\n      <td>Uanachama ulijumuisha  kati ya wanaume wazima ...</td>\n    </tr>\n    <tr>\n      <th>1397</th>\n      <td>ID_b871ea53_dev_afrixnli_hau</td>\n      <td></td>\n      <td>hau</td>\n      <td></td>\n      <td>Is the following question True, False or Neither?</td>\n      <td>Ban damu ba da abinda labarun Æ™asa ke nuni cik...</td>\n      <td>2</td>\n      <td>Ka damu da yadda labarun Æ™asa ke shafar unguwa...</td>\n    </tr>\n    <tr>\n      <th>1398</th>\n      <td>ID_70aae970_dev_afrixnli_hau</td>\n      <td></td>\n      <td>hau</td>\n      <td></td>\n      <td>Is the following question True, False or Neither?</td>\n      <td>Ya zabi Æ™in kama hannayen sa saboda anyi musu ...</td>\n      <td>0</td>\n      <td>Kuma mani rashin mutunci dan bazan goyi banyan...</td>\n    </tr>\n    <tr>\n      <th>1399</th>\n      <td>ID_9d731aca_dev_afrixnli_swa</td>\n      <td></td>\n      <td>swa</td>\n      <td></td>\n      <td>Is the following question True, False or Neither?</td>\n      <td>Steve Harris alikataa kutoka kwa nyumba yake k...</td>\n      <td>2</td>\n      <td>Steve Harris, mwana biolojia wa molekuli kutok...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1400 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def extract_task_from_id(id_string):\n    #Extract task type from ID string.\n    task = id_string.split('_')[3]\n    # Handle special case for sentiment task\n    return 'sentiment' if task == ' dev' else task\n\ndef balance_target_lengths(df, task_column='task', reference_task='mt', repetition_factor=11):\n    \"\"\"\n    Balance target sequence lengths by repeating shorter targets.\n    \n    Args:\n        df (DataFrame): DataFrame containing task and targets columns\n        task_column (str): Name of the task column\n        reference_task (str): Task with longer sequences to use as reference\n        repetition_factor (int): Number of times to repeat shorter sequences\n        \n    Returns:\n        DataFrame: DataFrame with balanced target lengths\n    \"\"\"\n    df_balanced = df.copy()\n    \n    for task in df_balanced[task_column].unique():\n        if task != reference_task:\n            mask = df_balanced[task_column] == task\n            df_balanced.loc[mask, 'targets'] = df_balanced.loc[mask, 'targets'].apply(\n                lambda x: ' '.join([x] * repetition_factor)\n            )\n    \n    return df_balanced\n\n\n\n#Format examples for instruction tuning.\ndef formatting_prompts_func(example):\n    premise = example['premise']\n    premise = premise+'\\n' if len(premise) else ''\n    if example['targets'] is not None:\n        return f\"### Instruction: {example['instruction']}\\n### Input: {premise}{example['inputs']}\\n### Response: {example['targets']}\"\n    return f\"### Instruction: {example['instruction']}\\n### Input: {premise}{example['inputs']}\\n### Response:\"\n\n\ndef setup_model_and_tokenizer(model_name, use_4bit=True):\n    \n    #Set up model and tokenizer for QLoRA fine-tuning if argument use_4bit = True.\n\n    # Define BitsAndBytes config for quantization\n    if use_4bit:\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n    else:\n        bnb_config = None\n    \n    # Load model with quantization config\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        token=token,\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    return model, tokenizer, bnb_config\n\n\ndef apply_lora_adapters(model, r=8, lora_alpha=16, dropout=0.05):\n    \n    # Define LoRA Config\n    lora_config = LoraConfig(\n        r=r,\n        lora_alpha=lora_alpha,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    # Apply LoRA adapters to model\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    \n    return model\n\n\ndef setup_trainer(model, dataset, tokenizer, output_dir, num_epochs=6):\n    \"\"\"\n    Set up SFTTrainer for direct fine-tuning.\n    \n    Args:\n        model: Model to fine-tune\n        dataset: Training dataset\n        tokenizer: Tokenizer\n        output_dir (str): Output directory for checkpoints\n        num_epochs (int): Number of training epochs\n        \n    Returns:\n        SFTTrainer: Trainer object\n    \"\"\"\n    # Define response template for proper label masking\n    response_template_with_context = \"\\n### Response:\"\n    response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]\n    \n    # Data collator for masked LM training\n    collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n    \n    # Training arguments\n    train_args = SFTConfig(\n        output_dir=output_dir,\n        max_seq_length=256,\n        num_train_epochs=num_epochs,\n        save_strategy=\"epoch\",\n        optim = 'adamw_bnb_8bit',\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        logging_steps=10,\n        save_total_limit=2,\n        report_to=[],  # Disable wandb\n    )\n    \n    # Trainer setup\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        args=train_args,\n        formatting_func=formatting_prompts_func,\n        data_collator=collator,\n    )\n    \n    return trainer\n\n\ndef generate_response(model, tokenizer, prompt, max_new_tokens=20):\n    \"\"\"\n    Generate response using fine-tuned model.\n    \n    Args:\n        model: Fine-tuned model\n        tokenizer: Tokenizer\n        prompt (str): Input prompt\n        max_new_tokens (int): Maximum number of tokens to generate\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=False\n        )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = generated_text[len(prompt):].strip()\n    \n    return response\n\n\ndef encode_sentiment_label(label):\n    \"\"\"\n    Encode sentiment label to integer.\n    \n    Args:\n        label (str): Sentiment label\n        \n    Returns:\n        int: Encoded label\n    \"\"\"\n    for c, i in enumerate([\"Chanya\", \"Wastani\", \"Hasi\"]):\n        if label == i:\n            return c\n    for c, i in enumerate([\"Kyakkyawa\", \"Tsaka-tsaki\", \"Korau\"]):\n        if label == i:\n            return c\n    return 0\n\n\ndef apply_inference_to_test_data(model, tokenizer, test_dataset):\n\n    df = pd.DataFrame(test_dataset)\n    model.eval()\n    \n    # Apply inference with tqdm progress bar\n    tqdm.pandas(desc=\"Generating Responses\")\n    df['generated'] = df.progress_apply(\n        lambda row: generate_response(model, tokenizer, formatting_prompts_func(row)), \n        axis=1\n    )\n    \n    # Process responses based on task type\n    df['Response'] = ''\n    \n    # Sentiment task\n    mask = df.ID.apply(lambda x: 'sentiment' in x)\n    df.loc[mask, 'Response'] = df.loc[mask, 'generated'].apply(\n        lambda x: encode_sentiment_label(x.strip().split()[0])\n    )\n    \n    # XNLI task\n    mask = df.ID.apply(lambda x: 'afrixnli' in x)\n    df.loc[mask, 'Response'] = df.loc[mask, 'generated'].apply(\n        lambda x: int(x.strip().split()[0])%3 if x.strip().split()[0].isdigit() else 0\n    )\n    \n    # MT task\n    mask = df.ID.apply(lambda x: 'mt_' in x)\n    df.loc[mask, 'Response'] = df.loc[mask, 'generated']\n    \n    return df\n\ndef display_formatted_examples(df, num_examples=2):\n    \"\"\"\n    Display formatted examples for each task.\n    \n    Args:\n        df (DataFrame): DataFrame containing the examples\n        num_examples (int): Number of examples to display per task\n    \"\"\"\n    for task in df.task.unique():\n        print(f\"\\n\\n{'='*40}\\nTask: {task}\\n{'='*40}\")\n        mask = df.task == task\n        for i, (_, row) in enumerate(df[mask].iterrows()):\n            if i >= num_examples:\n                break\n                \n            print(f\"\\nExample {i+1}:\")\n            print(\"-\" * 40)\n            formatted = formatting_prompts_func(row)\n            print(formatted)\n            print(\"-\" * 40)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:48.354334Z","iopub.execute_input":"2025-05-06T16:18:48.354548Z","iopub.status.idle":"2025-05-06T16:18:48.371358Z","shell.execute_reply.started":"2025-05-06T16:18:48.354529Z","shell.execute_reply":"2025-05-06T16:18:48.370482Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Load and explore the datasets","metadata":{}},{"cell_type":"code","source":"print(\"# Loading datasets\")\ntrain_dataset = load_and_combine_datasets(\"Train\")\ntest_dataset = load_and_combine_datasets(\"Test\")\n\nprint(\"\\n# Example from training dataset:\")\nprint(train_dataset[0])\n\nprint(\"\\n# Example from test dataset:\")\nprint(test_dataset[0])","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:48.372397Z","iopub.execute_input":"2025-05-06T16:18:48.372665Z","iopub.status.idle":"2025-05-06T16:18:54.606349Z","shell.execute_reply.started":"2025-05-06T16:18:48.372643Z","shell.execute_reply":"2025-05-06T16:18:54.605621Z"}},"outputs":[{"name":"stdout","text":"# Loading datasets\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/485 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff5511fd72843a581ed8a68d17d0fd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/39.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a0b74b7127419eaf1f7036261615c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aef806cd8c24c22b2d7d4fe9756e6e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/485 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa6ad374633439db861d9a13863bffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/72.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"123c4b33a7d3491fb3ca0c7fa3337e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df700dff9eec42d6bed0af0bd934a180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/447 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcd4f66f4dd4dc6ab0cb6cfa314fea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/35.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d1d2ec206c4be28e89836d39cb9257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0af1660643b4e1bbf34f3fc402e0736"}},"metadata":{}},{"name":"stdout","text":"All Columns: ['task', 'data_source', 'instruction', 'targets', 'premise', 'inputs', 'langs', 'ID']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2723c8da7fc947cdac57ec2f01424cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62e83dd882347f29bab948ca5f06455"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb74e7c5e674ed190e76cbc4401e460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/486 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf101a4bf59465da09ed4ba3f297595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/33.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6146470254f4ba3a5a96c218315de9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5300415f578143ecbdfc7aea26f9484f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/484 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d44d637a7e4707b1d974e90a15b840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/22.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee58e1ab73fc4a93986c9787774160ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46274d9dee3b450bbe3ed1df0bf98a38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/447 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558db5fbfa934e1da84a28daec2bb019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/28.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3a4c5b11e54984a184ace8d57c05fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b96650f8e04817aadad1e6f1e8d8e5"}},"metadata":{}},{"name":"stdout","text":"All Columns: ['task', 'data_source', 'instruction', 'targets', 'premise', 'inputs', 'langs', 'ID']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef40fc7e9d5d4b83956d2e219db65dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7118a668cd8646acb0c8a1614ba13ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b995826315ff4be3b5498be2bc9ef648"}},"metadata":{}},{"name":"stdout","text":"\n# Example from training dataset:\n{'ID': 'ID_6aba33a1_sentiment_ dev_hausa', 'task': 'sentiment', 'langs': 'hausa', 'data_source': 'afrisenti', 'instruction': 'Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.', 'inputs': '@user @user allah ya tsayyaba yar uwa ğŸ‚ ğŸ˜', 'targets': 'Kyakkyawa', 'premise': ''}\n\n# Example from test dataset:\n{'ID': 'ID_f3c74c7b_sentiment_test__hausa', 'task': 'sentiment', 'langs': 'hausa', 'data_source': 'afrisenti', 'instruction': \"Gano ra'ayin da aka bayyana a cikin wannan rubutu. Bin waÉ—annan jagororin, kyakkyawa yana na rubutu na nufin kyakkyawan tunani, É—abi'a, da motsin rai. Korau na nuna rubutu na nufin mummunan tunani ko motsin rai. Tsaka-tsaki na nuna rubutu baya nufin magana mai kyau ko mara kyau kai tsaye ko a kaikaice.\", 'inputs': '@user ynxu fha da kanada kudi shikenan duk kayan nan zasu iya zama naka noğŸ§¢', 'targets': None, 'premise': ''}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Convert dataset to DataFrame for easier processing","metadata":{}},{"cell_type":"code","source":"print(\"# Converting to DataFrame and extracting task types\")\ntrain_df = train_dataset.to_pandas()\ntrain_df['task'] = train_df.ID.apply(extract_task_from_id)\n\nprint(\"\\n# Dataset distribution by task:\")\nprint(train_df.task.value_counts())","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:54.608490Z","iopub.execute_input":"2025-05-06T16:18:54.608701Z","iopub.status.idle":"2025-05-06T16:18:54.624242Z","shell.execute_reply.started":"2025-05-06T16:18:54.608682Z","shell.execute_reply":"2025-05-06T16:18:54.623413Z"}},"outputs":[{"name":"stdout","text":"# Converting to DataFrame and extracting task types\n\n# Dataset distribution by task:\ntask\nmt           600\nsentiment    400\nafrixnli     400\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"display_formatted_examples(train_df)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:54.625265Z","iopub.execute_input":"2025-05-06T16:18:54.625556Z","iopub.status.idle":"2025-05-06T16:18:54.641517Z","shell.execute_reply.started":"2025-05-06T16:18:54.625534Z","shell.execute_reply":"2025-05-06T16:18:54.640661Z"}},"outputs":[{"name":"stdout","text":"\n\n========================================\nTask: sentiment\n========================================\n\nExample 1:\n----------------------------------------\n### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.\n### Input: @user @user allah ya tsayyaba yar uwa ğŸ‚ ğŸ˜\n### Response: Kyakkyawa\n----------------------------------------\n\nExample 2:\n----------------------------------------\n### Instruction: Da fatan za a gano ra'ayin da ke cikin wannan rubutu bisa ga jagorori masu zuwa: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.\n### Input: @user intenet a masallachiğŸ˜­ğŸ˜­ğŸ˜­ wani salo ne na karkatar da masu ibada zuwa wani abu daban amma a raayina bai da mahimmanchi\n### Response: Tsaka-tsaki\n----------------------------------------\n\n\n========================================\nTask: mt\n========================================\n\nExample 1:\n----------------------------------------\n### Instruction: could you convert this english text to swahili?\n### Input: he died on tuesday afternoon, less than a week after his diagnosis was announced, and on the same day that president ernest bai koroma was due to visit his treatment centre in the northeastern town of kailahun.\n### Response: kifo cha khan kimetokea wiki moja tu baada ya kugunduliwa na ugonjwa huo, na kutangazwa siku ile, ile rais wa sierra leone ernest bai koroma alikuwa anatarajiwa kutembelea kituo cha matibabu ya wagonjwa wa ebola cha mji wa kaskazini mashariki wa kailahun.\n----------------------------------------\n\nExample 2:\n----------------------------------------\n### Instruction: could you convert this english text to hausa?\n### Input: this is possible because the on-demand application is developed with such easy designs and functions.\n### Response: wannan yana yiwuwa ne saboda sauÆ™i na gyare-gyare a cikin kwakwalwar da ake so da kuma yin haka da kyau da kuma dacewa.\n----------------------------------------\n\n\n========================================\nTask: afrixnli\n========================================\n\nExample 1:\n----------------------------------------\n### Instruction: Is the following question True, False or Neither?\n### Input: Kuma kawai sai naji Hakane, wannan din ne.\nBayan nace e, ya Æ™are.\n### Response: 0\n----------------------------------------\n\nExample 2:\n----------------------------------------\n### Instruction: Is the following question True, False or Neither?\n### Input: Eh, Eh, na sani, ba zan ma damu sosai ba idan suna da kamfani dake da kudi\nBa zan damu ba idan É—aukan nauyin kuÉ—in da ake gudanar da ma'aikatar ake yi.\n### Response: 0\n----------------------------------------\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"\\n# Applying target length balancing fix\")\nbalanced_df = balance_target_lengths(train_df)\ndisplay_formatted_examples(balanced_df)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:54.642416Z","iopub.execute_input":"2025-05-06T16:18:54.642679Z","iopub.status.idle":"2025-05-06T16:18:54.661874Z","shell.execute_reply.started":"2025-05-06T16:18:54.642651Z","shell.execute_reply":"2025-05-06T16:18:54.661279Z"}},"outputs":[{"name":"stdout","text":"\n# Applying target length balancing fix\n\n\n========================================\nTask: sentiment\n========================================\n\nExample 1:\n----------------------------------------\n### Instruction: Za ka iya tantance yanayin wannan rubutu? Bi waÉ—annan jagororin sharhi: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.\n### Input: @user @user allah ya tsayyaba yar uwa ğŸ‚ ğŸ˜\n### Response: Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa\n----------------------------------------\n\nExample 2:\n----------------------------------------\n### Instruction: Da fatan za a gano ra'ayin da ke cikin wannan rubutu bisa ga jagorori masu zuwa: Kyakkyawa: idan rubutu na nuna kyakkyawan tunani, hali, da yanayi. Korau: idan rubutu yana nuna mummunar tunani ko yanayi. Neutral: idan rubutu baya nuna kyakkyawar magana ko mara kyau kai tsaye ko a kaikaice.\n### Input: @user intenet a masallachiğŸ˜­ğŸ˜­ğŸ˜­ wani salo ne na karkatar da masu ibada zuwa wani abu daban amma a raayina bai da mahimmanchi\n### Response: Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki\n----------------------------------------\n\n\n========================================\nTask: mt\n========================================\n\nExample 1:\n----------------------------------------\n### Instruction: could you convert this english text to swahili?\n### Input: he died on tuesday afternoon, less than a week after his diagnosis was announced, and on the same day that president ernest bai koroma was due to visit his treatment centre in the northeastern town of kailahun.\n### Response: kifo cha khan kimetokea wiki moja tu baada ya kugunduliwa na ugonjwa huo, na kutangazwa siku ile, ile rais wa sierra leone ernest bai koroma alikuwa anatarajiwa kutembelea kituo cha matibabu ya wagonjwa wa ebola cha mji wa kaskazini mashariki wa kailahun.\n----------------------------------------\n\nExample 2:\n----------------------------------------\n### Instruction: could you convert this english text to hausa?\n### Input: this is possible because the on-demand application is developed with such easy designs and functions.\n### Response: wannan yana yiwuwa ne saboda sauÆ™i na gyare-gyare a cikin kwakwalwar da ake so da kuma yin haka da kyau da kuma dacewa.\n----------------------------------------\n\n\n========================================\nTask: afrixnli\n========================================\n\nExample 1:\n----------------------------------------\n### Instruction: Is the following question True, False or Neither?\n### Input: Kuma kawai sai naji Hakane, wannan din ne.\nBayan nace e, ya Æ™are.\n### Response: 0 0 0 0 0 0 0 0 0 0 0\n----------------------------------------\n\nExample 2:\n----------------------------------------\n### Instruction: Is the following question True, False or Neither?\n### Input: Eh, Eh, na sani, ba zan ma damu sosai ba idan suna da kamfani dake da kudi\nBa zan damu ba idan É—aukan nauyin kuÉ—in da ake gudanar da ma'aikatar ake yi.\n### Response: 0 0 0 0 0 0 0 0 0 0 0\n----------------------------------------\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"balanced_dataset = Dataset.from_pandas(balanced_df.reset_index(drop=True))","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:54.662612Z","iopub.execute_input":"2025-05-06T16:18:54.662814Z","iopub.status.idle":"2025-05-06T16:18:54.690038Z","shell.execute_reply.started":"2025-05-06T16:18:54.662795Z","shell.execute_reply":"2025-05-06T16:18:54.689053Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_name = \"lelapa/InkubaLM-0.4B\"\n# turn off qlora: use_4bit=False \nmodel, tokenizer, bnb_config = setup_model_and_tokenizer(model_name, use_4bit=False)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:18:54.691206Z","iopub.execute_input":"2025-05-06T16:18:54.691479Z","iopub.status.idle":"2025-05-06T16:19:10.112027Z","shell.execute_reply.started":"2025-05-06T16:18:54.691457Z","shell.execute_reply":"2025-05-06T16:19:10.110975Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/763 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66be6229bf9141b18bdab531e6cdadd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.66G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa03bffe3ae44babd69677c5ffcc624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd9e633e5fa490ba67bd8158be714a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f7def5f61424adaa32b706d11f93bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/991k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a263fc618b418e86765ec108f6b648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.95M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85cbb895b1ff44aa8e0960f8a66c495c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13231d461b1540d7acb035d5bb10131c"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Train with balanced dataset","metadata":{}},{"cell_type":"code","source":"balanced_trainer = setup_trainer(\n    model=model, \n    dataset=balanced_dataset,\n    tokenizer=tokenizer,\n    output_dir=\"./sft_model/balanced\"\n)\nbalanced_trainer.train()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:19:10.113362Z","iopub.execute_input":"2025-05-06T16:19:10.113740Z","iopub.status.idle":"2025-05-06T16:42:45.728312Z","shell.execute_reply.started":"2025-05-06T16:19:10.113677Z","shell.execute_reply":"2025-05-06T16:42:45.727356Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Applying formatting function to train dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5dfa76d7a4e406398ee42ce1ab6db5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/1400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ebcd5a582b412ba06db6c14a5245dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b94811577284d4c83bedf9ade365703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f22eae16fb41e3a200e4b87476da64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/1400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd86587b4e64c77ab8ca32844a0a5bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='522' max='522' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [522/522 23:29, Epoch 5/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>18.346500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>18.690500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>16.272100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>17.981800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>15.136800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>13.081400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>17.713300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>15.458200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>14.525200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>9.629000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>10.459300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>10.197600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>9.542400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>8.363100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>8.242700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>8.143400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>10.400500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>5.862300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>4.764100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.600000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>4.037900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>4.696600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>4.747500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>4.372200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>4.753400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>4.850400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.978700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.273300</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.304100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.679000</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.686700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.999200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.132900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.017900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.488800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.766200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.511000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.566000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.543800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.665100</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.371800</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.580000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.175600</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.346700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.001700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.985800</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.272100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.788600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.646900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.021600</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.421700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.924600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=522, training_loss=5.922442252836921, metrics={'train_runtime': 1412.798, 'train_samples_per_second': 5.946, 'train_steps_per_second': 0.369, 'total_flos': 3286876516245504.0, 'train_loss': 5.922442252836921})"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"ckpt_path = \"sft_model/balanced/checkpoint-440/\"\ninference_model = AutoModelForCausalLM.from_pretrained(ckpt_path,\n                                                        device_map=\"auto\")\ninference_model.eval()\nresults_df = apply_inference_to_test_data(inference_model, tokenizer, test_dataset)\nresults_df[['ID', 'Response']].to_csv('submission_full_finetune.csv', index=False)\nresults_df[['ID', 'generated','Response']].head()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:47:26.429955Z","iopub.execute_input":"2025-05-06T16:47:26.430317Z","iopub.status.idle":"2025-05-06T16:51:07.460883Z","shell.execute_reply.started":"2025-05-06T16:47:26.430290Z","shell.execute_reply":"2025-05-06T16:51:07.460009Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating Responses:   0%|          | 0/900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99263c568c1a44ff82de9d77a59cb3cd"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                    ID  \\\n0    ID_f3c74c7b_sentiment_test__hausa   \n1    ID_aad19dbf_sentiment_test__hausa   \n2    ID_f6de0381_sentiment_test__hausa   \n3  ID_cbec84fe_sentiment_test__swahili   \n4    ID_885caf5c_sentiment_test__hausa   \n\n                                           generated Response  \n0    Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki        1  \n1  Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa        0  \n2           Korau Korau Korau Korau Korau Korau Kora        2  \n3  Wastani Wastani Wastani Wastani Wastani Wastan...        1  \n4           Korau Korau Korau Korau Korau Korau Kora        2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>generated</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_f3c74c7b_sentiment_test__hausa</td>\n      <td>Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki Tsaka-tsaki</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_aad19dbf_sentiment_test__hausa</td>\n      <td>Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa Kyakkyawa</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_f6de0381_sentiment_test__hausa</td>\n      <td>Korau Korau Korau Korau Korau Korau Kora</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_cbec84fe_sentiment_test__swahili</td>\n      <td>Wastani Wastani Wastani Wastani Wastani Wastan...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_885caf5c_sentiment_test__hausa</td>\n      <td>Korau Korau Korau Korau Korau Korau Kora</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}